{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LeNet_5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skks11/Colabresource/blob/master/LeNet_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "7EXukIx1cg9Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RuEX6xICciG1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm 'LeNet5_infernece (1).py'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9-tmC1CBcltl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm  LeNet5_infernece.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Immk-CD9coV1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "INPUT_NODE = 784\n",
        "OUTPUT_NODE = 10\n",
        "\n",
        "IMAGE_SIZE = 28\n",
        "NUM_CHANNELS = 1\n",
        "NUM_LABELS = 10\n",
        "\n",
        "CONV1_DEEP = 32\n",
        "CONV1_SIZE = 5\n",
        "\n",
        "CONV2_DEEP = 64\n",
        "CONV2_SIZE = 5\n",
        "\n",
        "FC_SIZE = 512\n",
        "def inference(input_tensor, train, regularizer):\n",
        "    #tf.reset_default_graph()\n",
        "    with tf.variable_scope('layer1-conv1',reuse=tf.AUTO_REUSE):\n",
        "        conv1_weights = tf.get_variable(\n",
        "            \"weight\", [CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP],\n",
        "            initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
        "        conv1_biases = tf.get_variable(\"bias\", [CONV1_DEEP], initializer=tf.constant_initializer(0.0))\n",
        "        conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
        "        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n",
        "\n",
        "    with tf.name_scope(\"layer2-pool1\"):\n",
        "        pool1 = tf.nn.max_pool(relu1, ksize = [1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
        "\n",
        "    with tf.variable_scope(\"layer3-conv2\",reuse=tf.AUTO_REUSE):\n",
        "        conv2_weights = tf.get_variable(\n",
        "            \"weight\", [CONV2_SIZE, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP],\n",
        "            initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
        "        conv2_biases = tf.get_variable(\"bias\", [CONV2_DEEP], initializer=tf.constant_initializer(0.0))\n",
        "        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
        "        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n",
        "\n",
        "    with tf.name_scope(\"layer4-pool2\"):\n",
        "        pool2 = tf.nn.max_pool(relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "        pool_shape = pool2.get_shape().as_list()\n",
        "        nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]\n",
        "        reshaped = tf.reshape(pool2, [pool_shape[0], nodes])\n",
        "\n",
        "    with tf.variable_scope('layer5-fc1',reuse=tf.AUTO_REUSE):\n",
        "        fc1_weights = tf.get_variable(\"weight\", [nodes, FC_SIZE],\n",
        "                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
        "        if regularizer != None: tf.add_to_collection('losses', regularizer(fc1_weights))\n",
        "        fc1_biases = tf.get_variable(\"bias\", [FC_SIZE], initializer=tf.constant_initializer(0.1))\n",
        "\n",
        "        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)\n",
        "        if train: fc1 = tf.nn.dropout(fc1, 0.5)\n",
        "\n",
        "    with tf.variable_scope('layer6-fc2',reuse=tf.AUTO_REUSE):\n",
        "        fc2_weights = tf.get_variable(\"weight\", [FC_SIZE, NUM_LABELS],\n",
        "                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
        "        if regularizer != None: tf.add_to_collection('losses', regularizer(fc2_weights))\n",
        "        fc2_biases = tf.get_variable(\"bias\", [NUM_LABELS], initializer=tf.constant_initializer(0.1))\n",
        "        logit = tf.matmul(fc1, fc2_weights) + fc2_biases\n",
        "\n",
        "    return logit\n",
        "  \n",
        "BATCH_SIZE = 100\n",
        "LEARNING_RATE_BASE = 0.01\n",
        "LEARNING_RATE_DECAY = 0.99\n",
        "REGULARIZATION_RATE = 0.0001\n",
        "TRAINING_STEPS = 8001\n",
        "MOVING_AVERAGE_DECAY = 0.99\n",
        "\n",
        "\n",
        "def train(mnist):\n",
        "    tf.reset_default_graph()\n",
        "    # 定义输出为4维矩阵的placeholder\n",
        "    x = tf.placeholder(tf.float32, [\n",
        "        BATCH_SIZE,\n",
        "        IMAGE_SIZE,\n",
        "        IMAGE_SIZE,\n",
        "        NUM_CHANNELS],\n",
        "                       name='x-input')\n",
        "    x_ = tf.placeholder(tf.float32, [\n",
        "        10000,\n",
        "        IMAGE_SIZE,\n",
        "        IMAGE_SIZE,\n",
        "        NUM_CHANNELS],\n",
        "                       name='x_-input')\n",
        "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')\n",
        "  \n",
        "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
        "    y = inference(x, False, regularizer)\n",
        "    global_step = tf.Variable(0, trainable=False)\n",
        "\n",
        "    # 定义损失函数、学习率、滑动平均操作以及训练过程。\n",
        "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
        "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
        "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
        "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
        "    loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
        "    \n",
        "    \n",
        "    \n",
        "    learning_rate = tf.train.exponential_decay(\n",
        "        LEARNING_RATE_BASE,\n",
        "        global_step,\n",
        "        mnist.train.num_examples / BATCH_SIZE, LEARNING_RATE_DECAY,\n",
        "        staircase=True)\n",
        "\n",
        "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
        "    with tf.control_dependencies([train_step, variables_averages_op]):\n",
        "        train_op = tf.no_op(name='train')\n",
        "\n",
        "    y1=inference(x_,None,regularizer)\n",
        "    prediction=tf.equal(tf.argmax(y1,1),tf.argmax(y_,1))\n",
        "    acc=tf.reduce_mean(tf.cast(prediction,tf.float32))\n",
        "    \n",
        "    # 初始化TensorFlow持久化类。\n",
        "    saver = tf.train.Saver()\n",
        "    with tf.Session() as sess:\n",
        "        tf.global_variables_initializer().run()\n",
        "        for i in range(TRAINING_STEPS):\n",
        "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
        "            #print(xs.shape)\n",
        "            #print(mnist.test.images.shape)\n",
        "            reshaped_xs_ = np.reshape(mnist.test.images, (\n",
        "                10000,\n",
        "                IMAGE_SIZE,\n",
        "                IMAGE_SIZE,\n",
        "                NUM_CHANNELS))\n",
        "            reshaped_xs = np.reshape(xs, (\n",
        "                BATCH_SIZE,\n",
        "                IMAGE_SIZE,\n",
        "                IMAGE_SIZE,\n",
        "                NUM_CHANNELS))\n",
        "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: reshaped_xs, y_: ys})\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "                print(\"After %d training step(s), loss on training batch is %g.\" % (step, loss_value))\n",
        "                print(\"acc:\",sess.run(acc,feed_dict={x_: reshaped_xs_, y_: mnist.test.labels}))\n",
        "\n",
        "def main(argv=None):\n",
        "    mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
        "    train(mnist)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bz0o4uihc7QC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import LeNet5_infernece\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}